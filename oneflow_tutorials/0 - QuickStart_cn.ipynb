{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febd4dc3",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad612c7a",
   "metadata": {},
   "source": [
    "在这部分中，我们提供了一些基本但全面的示例，展示了Torch-Pruning的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b0a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import oneflow as torch\n",
    "from flowvision.models import resnet18\n",
    "import oneflow_pruning as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6df445",
   "metadata": {},
   "source": [
    "### 方法1. 依赖图剪枝\n",
    "\n",
    "``DependencyGraph`` : 是Torch-Pruning的基石，它自动识别和分组所有具有相互依赖关系的层。在结构剪枝中，具有依赖关系的两个层应同时进行剪枝。因此，要剪枝一个复杂的模型，我们需要仔细处理这些层。下面的示例显示了在ResNet-18中剪枝单个层的流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef2915f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. 选择要修剪的一些通道。这里我们修剪索引为[2, 6, 9]的通道。\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pruning_idxs \u001b[38;5;241m=\u001b[39m pruning_idxs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m pruning_group \u001b[38;5;241m=\u001b[39m \u001b[43mDG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pruning_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_conv_out_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruning_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 3. 修剪与model.conv1耦合的所有分组层\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DG\u001b[38;5;241m.\u001b[39mcheck_pruning_group(pruning_group):\n",
      "File \u001b[0;32m~/OneFlow-Pruning/oneflow_pruning/dependency.py:390\u001b[0m, in \u001b[0;36mDependencyGraph.get_pruning_group\u001b[0;34m(self, module, pruning_fn, idxs)\u001b[0m\n\u001b[1;32m    388\u001b[0m group \u001b[38;5;241m=\u001b[39m Group()\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m#  the user pruning operation\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m root_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule2node\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    391\u001b[0m group\u001b[38;5;241m.\u001b[39madd_dep(\n\u001b[1;32m    392\u001b[0m     Dependency(pruning_fn, pruning_fn,\n\u001b[1;32m    393\u001b[0m                source\u001b[38;5;241m=\u001b[39mroot_node, target\u001b[38;5;241m=\u001b[39mroot_node), idxs\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    396\u001b[0m visited_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyError\u001b[0m: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
     ]
    }
   ],
   "source": [
    "# 0. 准备模型和示例输入\n",
    "model = resnet18(pretrained=True).eval()\n",
    "example_inputs = torch.randn(1,3,224,224)\n",
    "\n",
    "# 1. 为resnet18构建依赖图\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "# 2. 选择要修剪的一些通道。这里我们修剪索引为[2, 6, 9]的通道。\n",
    "pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "pruning_group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "\n",
    "# 3. 修剪与model.conv1耦合的所有分组层\n",
    "if DG.check_pruning_group(pruning_group):\n",
    "    pruning_group.prune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0517ce0",
   "metadata": {},
   "source": [
    "在调用``.exec``方法后，将对模型进行原地剪枝。打印模型后，我们可以注意到多个层，例如“model.conv1”，“model.bn1”和“model.layer1[0].conv1”都被Torch-Pruning剪枝了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After pruning:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3ee9a",
   "metadata": {},
   "source": [
    "让我们检查剪枝组。结果将显示剪枝操作如何触发（=>）另一个操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pruning_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573e540",
   "metadata": {},
   "source": [
    "您还可以使用“get_all_groups”方法从“DependencyGraph”中获取所有组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b564ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups = list(DG.get_all_groups())\n",
    "print(\"Number of Groups: %d\"%len(all_groups))\n",
    "print(\"The last Group:\", all_groups[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9a76f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13e0b8",
   "metadata": {},
   "source": [
    "### 2. 高级剪枝器\n",
    "\n",
    "使用“DependencyGraph”对神经网络进行剪枝仍然可能很复杂，特别是对于具有众多层的模型。\n",
    "\n",
    "因此，我们还提供了高级剪枝器来简化此过程。\n",
    "\n",
    "例如，您可以使用简单的基于幅度的剪枝器轻松地剪枝ResNet18模型。\n",
    "\n",
    "该方法删除网络中幅度较小的权重，从而得到一个更小、更快的模型，而准确性损失不会太大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05fd44b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mresnet18\u001b[49m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 0. importance criterion for parameter selections\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 定义参数选择的重要性标准\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet18' is not defined"
     ]
    }
   ],
   "source": [
    "model = resnet18(pretrained=True)\n",
    "example_inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# 0. importance criterion for parameter selections\n",
    "# 定义参数选择的重要性标准\n",
    "imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')\n",
    "\n",
    "# 1. ignore some layers that should not be pruned, e.g., the final classifier layer.\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "        \n",
    "# 2. Pruner initialization\n",
    "# 剪枝器初始化\n",
    "iterative_steps = 5 # You can prune your model to the target sparsity iteratively.\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model, \n",
    "    example_inputs, \n",
    "    global_pruning=False, # If False, a uniform sparsity will be assigned to different layers.\n",
    "    importance=imp, # importance criterion for parameter selection\n",
    "    iterative_steps=iterative_steps, # the number of iterations to achieve target sparsity\n",
    "    ch_sparsity=0.5, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "# 计算模型的基本计算量和参数数量\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "for i in range(iterative_steps):\n",
    "    # 3. the pruner.step will remove some channels from the model with least importance\n",
    "    # pruner.step 将从具有最小重要性的模型中删除一些通道\n",
    "    pruner.step()\n",
    "    \n",
    "    # 4. Do whatever you like here, such as fintuning\n",
    "    # 在此处进行微调等操作\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(model)\n",
    "    print(model(example_inputs).shape)\n",
    "    print(\n",
    "        \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "        % (i+1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "    )\n",
    "    print(\n",
    "        \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "        % (i+1, iterative_steps, base_macs / 1e9, macs / 1e9)\n",
    "    )\n",
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    # ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffac08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
